# -*- coding: utf-8 -*-
"""EWC_GSG_SUMMER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ZGyMvOHzTnM7B0G_uArJCSrw2ab-sBD
"""

from google.colab import drive
drive.mount('/content/drive')

pip install datasets evaluate transformers rouge-score nltk

!apt install git-lfs

import transformers
from transformers.utils import send_example_telemetry

print(transformers.__version__)
send_example_telemetry("summarization_notebook", framework="pytorch")

from evaluate import load
rouge_metric = load('rouge')
bleu_metric = load('bleu')

from transformers import AutoTokenizer
from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer

model_path = '/content/drive/My Drive/Model_T5_GSG_Epoch/model_epoch_18.0'
tokenizer = AutoTokenizer.from_pretrained(model_path)

model_T5 = AutoModelForSeq2SeqLM.from_pretrained(model_path).to('cuda')

def replace_masks_with_extra_ids(text):
    """
    Replace each [MASK] token with T5 special tokens like <extra_id_0>, <extra_id_1>, etc.
    """
    mask_tokens = text.count('[MASK]')
    for i in range(mask_tokens):
        text = text.replace('[MASK]', f'<extra_id_{i}>', 1)  # Replace each [MASK] in sequence
    return text

from datasets import load_dataset, DatasetDict
import pandas as pd

data = pd.read_csv("/content/drive/My Drive/GSG.csv")  # Replace with the actual path to your CSV
data = data[['input', 'target']]


data['input'] = data['input'].apply(replace_masks_with_extra_ids)
data['target'] = data['target'].apply(lambda x: x.strip())


print(data[['input', 'target']].head(2))

from datasets import Dataset

# Load the dataset from the CSV file
dataset = Dataset.from_pandas(data[['input', 'target']])

# Split the dataset into 80% train, 10% validation, and 10% test
split_dataset = dataset.train_test_split(test_size=0.2)
test_validation_split = split_dataset['test'].train_test_split(test_size=0.5)

# Create a DatasetDict
dataset_dict = DatasetDict({
    'train': split_dataset['train'],
    'validation': test_validation_split['train'],
    'test': test_validation_split['test']
})


# Display the DatasetDict structure
print(dataset_dict)

max_input_length = 256
max_target_length = 256

def preprocess_function(examples):
    prefix = "fill in the blank: "  # Prefix for GSG task
    inputs = [prefix + doc for doc in examples["input"]]  # Add prefix to each input

    # Tokenize the inputs with truncation and padding
    model_inputs = tokenizer(
        inputs,
        max_length=max_input_length,
        truncation=True,
        padding="max_length"
    )

    # Tokenize the target (label) with truncation and padding
    labels = tokenizer(
        examples["target"],
        max_length=max_target_length,
        truncation=True,
        padding="max_length"
    )

    # Assign tokenized labels to model_inputs
    model_inputs["labels"] = labels["input_ids"]

    return model_inputs

tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)

# Check a sample from the tokenized dataset
# Explicitly remove 'input' and 'target' fields from the tokenized dataset
tokenized_datasets = tokenized_datasets.remove_columns(['input', 'target'])
print(tokenized_datasets['train'][0])

import torch
from torch.utils.data import DataLoader
from transformers import DataCollatorForSeq2Seq

# Data collator for padding and tensor conversion
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_T5, return_tensors="pt")  #Collate function to convert to tensors
# Create DataLoader for GSG task
dataloader_gap = DataLoader(tokenized_datasets['train'], batch_size=32,collate_fn=data_collator)
print(dataloader_gap.dataset)

# Print types and shapes of tensors in the batch
for batch in dataloader_gap:
    for key, value in batch.items():
        print(f"{key}: {type(value)}, shape: {value.shape if isinstance(value, torch.Tensor) else 'N/A'}")
    break  # Only print the first batch

print(dataloader_gap.dataset)
print(dataloader_gap.batch_size)
print(dataloader_gap.dataset[0])



import torch
from tqdm import tqdm  # Import tqdm for the progress bar

class EWC:
    def __init__(self, model, dataloader, device='cuda'):
        self.model = model
        self.device = device
        self.dataloader = dataloader
        self.params = {name: p.clone().detach() for name, p in self.model.named_parameters()}
        self.fisher = self._compute_fisher()

    def _compute_fisher(self):
        fisher = {name: torch.zeros(p.size()).to(self.device) for name, p in self.model.named_parameters()}
        self.model.eval()

        # Get the total number of batches to compute progress as a percentage
        total_batches = len(self.dataloader)

        # Initialize the progress bar (from 0 to 100%)
        progress_bar = tqdm(total=100, desc="Computing Fisher Matrix", bar_format="{l_bar}{bar}{r_bar}")

        # Process each batch
        for i, batch in enumerate(self.dataloader):
            # Move input data to GPU (if available)
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)

            self.model.zero_grad()  # Clear previous gradients
            outputs = self.model(input_ids=input_ids, labels=labels)
            loss = outputs.loss

            loss.backward()  # Backpropagate to compute gradients

            # Accumulate the squared gradients for Fisher calculation
            for name, param in self.model.named_parameters():
                if param.grad is not None:
                    fisher[name] += (param.grad ** 2)

            # Update progress bar based on the percentage of completed batches
            progress = (i + 1) / total_batches * 100  # Calculate percentage
            progress_bar.n = int(progress)  # Update progress value
            progress_bar.refresh()

        # Close the progress bar after computation
        progress_bar.close()

        # Normalize Fisher Information by number of batches in the dataloader
        fisher = {name: f / len(self.dataloader) for name, f in fisher.items()}
        return fisher

# Compute Fisher Information Matrix on GSG task
ewc = EWC(model_T5, dataloader_gap)

# Save the Fisher matrix
fisher_save_path = '/content/drive/My Drive/Model_T5_GSG_Epoch/fisher_matrix.pth'
torch.save(ewc.fisher, fisher_save_path)

# Save the model parameters
params_save_path = '/content/drive/My Drive/Model_T5_GSG_Epoch/params.pth'
torch.save(ewc.params, params_save_path)

print("Fisher matrix and model parameters saved successfully!")

# Assuming you already have the model and dataloader prepared
class EWC:
    def __init__(self, model, fisher, params, device='cuda'):
        self.model = model
        self.device = device
        self.fisher = fisher  # Use the loaded Fisher matrix
        self.params = params  # The parameters saved from the earlier task

# Initialize EWC with the loaded Fisher matrix
ewc = EWC(model_T5, fisher=fisher_matrix, params={name: p.clone().detach() for name, p in model_T5.named_parameters()})

# Now use this EWC object for regularization in the trainer

# Load the Fisher matrix
fisher_load_path = '/content/drive/My Drive/fisher_matrix.pth'
fisher_matrix = torch.load(fisher_load_path)

# Load the model parameters
params_load_path = '/content/drive/My Drive/params.pth'
model_params = torch.load(params_load_path)

# Use the loaded Fisher matrix and parameters
ewc = EWC(model_T5, fisher=fisher_matrix, params=model_params)

print("Fisher matrix and model parameters loaded successfully!")

print(ewc.fisher)

print(ewc.params)

